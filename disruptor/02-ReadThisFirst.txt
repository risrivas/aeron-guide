###########
# Abstract
###########
https://lmax-exchange.github.io/disruptor/disruptor.html

- develop a very high performant exchange or matching engine
- basic caveat in producer/consumer pattern is using queues
- how modern CPU's work - this is mechanical sympathy
- ideal for any "asynchronous event processing" architecture where high-throughput and low-latency is required
- LMAX has built - order matching engine, real-time risk management, HA in-memory transaction processing system
- Disruptor has less write contention, lower concurrency overhead and more cache friendly
all of which results in greater throughput with less jitter at lower latency
- maximum: 25 million messages per second and latencies lower than 50 nanoseconds
- very close to the theoretical limit of a modern processor to exchange data between cores


###########
# Overview
###########
- derived from designs of SEDA and Actor - use pipelines for throughput
https://github.com/mdwelsh/mdwelsh.github.io/blob/master/papers/seda-sosp01.pdf
http://dspace.mit.edu/handle/1721.1/6952

- Queues are the bottleneck


##############################
# Complexities of Concurrency
##############################
- parallel tasks contending for shared resources: database, file, socket, location in memory
- concurrent execution:
mutual exclusion = managing contended "updates" to some resource
visibility of change = controlling when such changes are made visible to "other threads"

- possible to remove the need of mutual exclusion if only 1 thread is updating
- The most costly operation in any concurrent environment is a "contended write access"
- if multiple threads try to update / write - then we need locking

## Costs of Locks
- locks provide mutual exclusion
- very expensive if context switch happens as all the cached data and instructions are lost

## Costs of "CAS"
- better than locks as no context switch happens
- CPU has CAS operation - "lock cmpxchg"
- allows a word in memory to be "updated" in atomic way
- expected value and to-update value are provided as arguments
- caveat: CPU must "lock its instruction pipeline" to ensure atomicity and employ a "memory barrier" to make the changes visible to other threads
- its complex to implement

- ideal algorithm: single thread owning all writes to a single resource while other threads just reading the result

##################
# Memory Barriers
##################
- to read the results in a multi-threaded / multi-core / multi-processor,
it requires "memory barrier" to make the changes visible to threads running on "other" processors

- CPUs perform this for achieving maximum performnace:
out-of-order execution of instructions
out-of-order loads and stores of data between memory and execution units

- when multiple threads share state, then memory changes should appear in order for data exchange to be successful
- thats where CPUs use memory barrier for section of codes that requires memory ordering => hardware barriers
- compilers can also put "software barriers" to ensure ordering of compiled code
- CPUs are much faster than memory systems
- thus, CPUs use complex cache system - fast hardware cache tables without chaining
- "cache coherency" amongst other CPUs using "message passing" protocols

- CPUs have "store buffers" to offload writes to these caches
- CPUs have "invalidate queues" which helps to invalidate cache used by "cache coherency" protocols

- Latest updated data can be anywhere
register
store buffer
one of many layers of cache (L1 / L2 / L3)
main memory (RAM)

- for making this data visible to other threads - this is done by coordinated exchange of cache coherency messages
- timely generation of these messages is controlled by memory barriers

# Read memory barrier
- orders load instructions on the CPU that executes it by marking a point in the "invalidate queue" for changes coming into its own cache
- this guarantees the latest updated value which was written "before" the read barrier

# Write memory barrier
- orders store instructions on the CPU that executes it by marking a point in the "store buffer" thus flushing writes out via its own cache
- this guarantees of what store operations happened "before" the write barrier

# Full memory barrier
- orders both load and store instructions on the CPU that executes it

- In Java, read/write of volatile field implements the read and write barriers respectively






